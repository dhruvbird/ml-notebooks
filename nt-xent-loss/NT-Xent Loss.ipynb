{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b4fad2",
   "metadata": {},
   "source": [
    "# Normalized Temperature-scaled Cross Entropy Loss (NT-Xent)\n",
    "\n",
    "https://paperswithcode.com/method/nt-xent\n",
    "\n",
    "### The setup\n",
    "\n",
    "This loss function is applied to N distributions (N being the batch size).\n",
    "\n",
    "The 1st and 2nd distributions in the set are considered to be similar to each other (as used in SimCLR for self-supervised learning). The rest are to be considered to be dissimilar to the 1st one.\n",
    "\n",
    "### Simple interpretation\n",
    "\n",
    "NT-Xent is just a very fancy way to say the following:\n",
    "\n",
    "1. An all-pairs Cosine Similarity score is computed for each of the N values.\n",
    "2. Comparison results between the same value are discarded (since a distribution is perfectly similar to itself)\n",
    "3. Each value (cosine similarity) is scaled by a temperature `T` (which is a hyper-parameter)\n",
    "4. Cross Entropy Loss is applied to each row of the resulting matrix above\n",
    "5. Typically, the mean of these losses (one per row) is used for backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "afd662fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "_ = torch.manual_seed(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5d390c",
   "metadata": {},
   "source": [
    "In this example, we assume that the input (and output) batch size is 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c2dd401b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 1.]]),\n",
       " tensor([[False,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True, False,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True, False,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True, False,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True, False,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True, False,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True, False,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True, False]]))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eye = torch.eye(8)\n",
    "eye, ~eye.bool()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae12351",
   "metadata": {},
   "source": [
    "Let's assume that the model's output is an `(8, 2)` tensor, with each element being a 2d vector in space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "577c3e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2386, -1.0934],\n",
       "        [ 0.1558,  0.1750],\n",
       "        [-0.9526, -0.5442],\n",
       "        [ 1.1985,  0.9604],\n",
       "        [-1.1074, -0.8403],\n",
       "        [-0.0020,  0.2240],\n",
       "        [ 0.8766, -0.5379],\n",
       "        [-0.2994,  0.9785]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(8, 2)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf3f759",
   "metadata": {},
   "source": [
    "Let's compute the cosine similarity between every pair of vectors. Since we have 8 vectors, we expect `8x8=64` similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "541f46d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000, -0.8714,  0.6698, -0.7773,  0.7604, -0.9751,  0.3293, -0.8719],\n",
       "        [-0.8714,  1.0000, -0.9479,  0.9860, -0.9812,  0.7408,  0.1763,  0.5195],\n",
       "        [ 0.6698, -0.9479,  1.0000, -0.9878,  0.9916, -0.4883, -0.4806, -0.2203],\n",
       "        [-0.7773,  0.9860, -0.9878,  1.0000, -0.9997,  0.6183,  0.3381,  0.3696],\n",
       "        [ 0.7604, -0.9812,  0.9916, -0.9997,  1.0000, -0.5973, -0.3628, -0.3449],\n",
       "        [-0.9751,  0.7408, -0.4883,  0.6183, -0.5973,  1.0000, -0.5306,  0.9588],\n",
       "        [ 0.3293,  0.1763, -0.4806,  0.3381, -0.3628, -0.5306,  1.0000, -0.7495],\n",
       "        [-0.8719,  0.5195, -0.2203,  0.3696, -0.3449,  0.9588, -0.7495,  1.0000]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xcs = F.cosine_similarity(x[None,:,:], x[:,None,:], dim=-1)\n",
    "xcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d83beca",
   "metadata": {},
   "source": [
    "Let's replace all the diagonal elements (cosine similarity of an element with itself) with `-inf` so that when we compute the softmax later, it will show up as `0.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "29b5ec4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   -inf, -0.8714,  0.6698, -0.7773,  0.7604, -0.9751,  0.3293, -0.8719],\n",
       "        [-0.8714,    -inf, -0.9479,  0.9860, -0.9812,  0.7408,  0.1763,  0.5195],\n",
       "        [ 0.6698, -0.9479,    -inf, -0.9878,  0.9916, -0.4883, -0.4806, -0.2203],\n",
       "        [-0.7773,  0.9860, -0.9878,    -inf, -0.9997,  0.6183,  0.3381,  0.3696],\n",
       "        [ 0.7604, -0.9812,  0.9916, -0.9997,    -inf, -0.5973, -0.3628, -0.3449],\n",
       "        [-0.9751,  0.7408, -0.4883,  0.6183, -0.5973,    -inf, -0.5306,  0.9588],\n",
       "        [ 0.3293,  0.1763, -0.4806,  0.3381, -0.3628, -0.5306,    -inf, -0.7495],\n",
       "        [-0.8719,  0.5195, -0.2203,  0.3696, -0.3449,  0.9588, -0.7495,    -inf]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = xcs.clone()\n",
    "y[eye.bool()] = float(\"-inf\")\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1fd837",
   "metadata": {},
   "source": [
    "Before we jump into using Cross Entropy loss to compute the loss, let's visualize the ground truth labels we wish to assign to every element in the cosine similarity matrix above.\n",
    "\n",
    "The matrix `target` below will contain the ground-truth labels we will feed in to the `cross_entropy` function to compute the cross entropy loss. However, when visualizing it, we wish to view it as a 2d matrix, and not a 1d matrix of labels per element.\n",
    "\n",
    "When interpreting the matrix `y` above, we need to keep in mind that the first row represents the Cosine Similarity between the element at index 0 and at every index between 0 and 7. The second row represents the Cosine Similarity between the element at index 1 and at every index between 0 and 7, and so on.\n",
    "\n",
    "The ground truth labels can be visualized as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b58d685e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 3, 2, 5, 4, 7, 6])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.arange(8)\n",
    "target[0::2] += 1\n",
    "target[1::2] -= 1\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f287ff4",
   "metadata": {},
   "source": [
    "The tensor `index` is going to be a column tensor (1 row, 8 columns) with the same elements as `target` that will be used to scatter the ones into the zeros 2d tensor `ground_truth_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a49eace5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [0],\n",
       "        [3],\n",
       "        [2],\n",
       "        [5],\n",
       "        [4],\n",
       "        [7],\n",
       "        [6]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = target.reshape(8, 1).long()\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4d9c8678",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_labels = torch.zeros(8, 8).long()\n",
    "src = torch.ones(8, 8).long()\n",
    "ground_truth_labels = torch.scatter(ground_truth_labels, 1, index, src)\n",
    "ground_truth_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d03eaa9",
   "metadata": {},
   "source": [
    "What follows is a more detailed explanation of why we create `target` the way we do above.\n",
    "\n",
    "If we wish to compute the row (or column) wise cross-entropy-loss, we observe that element at index 1 in the first row (index 0) is expected to have label 1 (for similar) and the rest of the values are dis-similar from the 0th indexed result. This is because the elements at index 0 and 1 are expected to be augmentations of the same image when using SimCLR.\n",
    "\n",
    "For the 2nd row (index 1), the element at index 0 should have the label 1 (mirror-image case of the one above), and the rest are dissimilar to the element at index 1.\n",
    "\n",
    "Similarly, we observe that the elements 3, 2, 5, 4, 7, 6 will have a label `1`.\n",
    "\n",
    "The tensor `y` contains the ground-truth labels for feeding into the cross entropy loss function.\n",
    "\n",
    "Here's what the documentation says about the [cross_entropy](https://pytorch.org/docs/stable/generated/torch.nn.functional.cosine_similarity.html) function:\n",
    "\n",
    "> This criterion computes the cross entropy loss between input logits and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c3d696f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8555)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(\n",
    "    y,\n",
    "    target,\n",
    "    reduction=\"mean\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a4b535",
   "metadata": {},
   "source": [
    "## Final implementation\n",
    "\n",
    "Now that we have seen how to build up the NT-Xent loss, we can condense the complete implementation (including the computation of the all-pairs Cosine Similarity) into this one simple function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "47dbc4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature:  0.01, Loss: 167.33396911621094\n",
      "Temperature:  0.10, Loss: 16.916988372802734\n",
      "Temperature:  1.00, Loss: 2.8555006980895996\n",
      "Temperature: 10.00, Loss: 2.0152008533477783\n",
      "Temperature: 20.00, Loss: 1.979940414428711\n"
     ]
    }
   ],
   "source": [
    "def nt_xent_loss(x, temperature):\n",
    "    assert len(x.size()) == 2\n",
    "    \n",
    "    # Cosine similarity\n",
    "    xcs = F.cosine_similarity(x[None,:,:], x[:,None,:], dim=-1)\n",
    "    xcs[torch.eye(x.size(0)).bool()] = float(\"-inf\")\n",
    "\n",
    "    # Ground truth labels\n",
    "    target = torch.arange(8)\n",
    "    target[0::2] += 1\n",
    "    target[1::2] -= 1\n",
    "    \n",
    "    # Standard cross entropy loss\n",
    "    return F.cross_entropy(xcs / temperature, target, reduction=\"mean\")\n",
    "\n",
    "for t in (0.01, 0.1, 1.0, 10.0, 20.0):\n",
    "    print(f\"Temperature: {t:5.2f}, Loss: {nt_xent_loss(x, temperature=t)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
